
# Project: SimpleLocalAIChat  

## Infos  
Llama-cpp-python is used to run the language model: https://github.com/abetlen/llama-cpp-python.  
Eel is used so that the Python code can communicate with the frontend: https://github.com/python-eel/Eel.  

The standard language model is Llama3-Instruct: https://ai.meta.com/blog/meta-llama-3/.  
A quantized version is used, which can be run with 8 GB of graphics memory:
https://huggingface.co/lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF.  
  
The model uses markdown for formatting. marked.js is used to convert to HTML tags: https://marked.js.org/.  
  
The output of the language model is output token by token during generation.  

<br>  
  
  


## Video   
https://github.com/Snens98/LocalChat/assets/116456908/0fbc7673-d1e5-4c38-9ed1-3a65c29e36ea  


![Screen](https://github.com/Snens98/LocalChat/assets/116456908/cdbb4cca-36cc-4112-945d-6f11a9ae8190)  


![Screenshot 2024-05-06 155005](https://github.com/Snens98/LocalChat/assets/116456908/49be48db-b90f-4f61-99b3-280212ce017f)  


![Screenshot 2024-05-06 155229](https://github.com/Snens98/LocalChat/assets/116456908/77dc35f7-d23c-4395-b05d-035cd64fac9a)  
